# Architecture:
  Mora is a multi-agent framework designed for video generation tasks. It utilizes a combination of text-to-image, image-to-image, image-to-video, and video-to-video agents to generate and edit videos. The text-to-image generation employs the Stable Diffusion XL (SDXL) model, which features an enlarged UNet backbone and a dual text encoder system for a nuanced interpretation of textual inputs. Image-to-image generation uses InstructPix2Pix, which integrates GPT-3 for generating editing instructions and Stable Diffusion for visual outputs.

# Dataset:
 highlights challenges in collecting high-quality video datasets due to copyright restrictions. Videos, unlike images, are more difficult to gather legally for training purposes. High-quality videos often come from copyrighted sources like movies, TV shows, and proprietary game footage.

# Drawbacks and Limitations:
Mora struggles with generating lifelike human movements and faces challenges in video quality and length. It has difficulty interpreting and rendering motion dynamics from prompts and cannot control the direction of motion. Videos generated by Mora degrade in quality beyond 12 seconds, and the system lacks the capability to follow complex instructions.

# Performance:
 Mora demonstrates competitive performance in video generation tasks, ranking second only to Sora in text-to-video generation. It achieves high scores in metrics like Object Consistency, Background Consistency, and Motion Smoothness. However, it falls short in quality and temporal style consistency compared to Sora. Mora's performance is evaluated using both basic metrics from Vbench and self-defined metrics across six tasks, showcasing its versatility and general capabilities in video generation.


<img width="878" alt="Screenshot 2024-06-25 at 12 55 24" src="https://github.com/usha3211-coder/Research-Development/assets/150019156/d259c14d-ba69-439f-8a1c-fd0efc891f68">


1. Text Prompt: The user provides  a textual description of their desired video. This description becomes the initial input for the system.
2. Prompt Selection & Generation : In some implementations, this agent utilizes advanced language models like GPT-4 to refine the user's prompt. The goal is to make it more detailed and informative for the subsequent video generation stages. This refined prompt is not always used, but it can be beneficial for complex videos.
3. Text-to-Image Generation: For intricate video generation tasks, a separate agent can be employed. It utilizes models like DALL-E 2 or SDXL to create an initial image based on the user's prompt (or the enhanced prompt if used in the previous step). This step provides a visual starting point for the video generation process.
4. Image-to-Video Generation: The core agent for video generation takes center stage. It can work with either:
The user prompt directly (if the Text-to-Image Generation stage was skipped).
The image generated by the previous agent (if it was used).
Models like SVD,Gen-2 or Emu are then used to transform the input – whether text or image – into a video sequence. This stage ensures that the video maintains temporal stability and visual consistency throughout its duration.
5. Optional Prompt Guidance : Even if the Text-to-Image Generation stage wasn't used, the enhanced prompt (if generated) might still be provided alongside the main input. This additional information can guide the video generation process and ensure better alignment with the user's original description.
6. Output Video: Finally, the user receives the generated video, which corresponds to their initial textual prompt. The Mora framework has successfully brought their vision to life.


# Reference code
visit https://github.com/lichao-sun/Mora
